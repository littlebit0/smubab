# Crawler ì—…ë°ì´íŠ¸ ê°€ì´ë“œ

## ğŸ”§ ì‹¤ì œ ìƒëª…ëŒ€í•™êµ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ êµ¬í˜„í•˜ê¸°

### 1. ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„

```bash
# ì›¹ì‚¬ì´íŠ¸ HTML êµ¬ì¡° í™•ì¸
curl -s "https://www.smu.ac.kr/kor/life/restaurantView.do" | head -100
```

ë¸Œë¼ìš°ì € ê°œë°œì ë„êµ¬ë¡œ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬:
- ë©”ë‰´ í…Œì´ë¸”/ë¦¬ìŠ¤íŠ¸ì˜ í´ë˜ìŠ¤ëª…
- ì‹ë‹¹ëª… ìœ„ì¹˜
- ë©”ë‰´ ì•„ì´í…œ êµ¬ì¡°
- ê°€ê²© ì •ë³´ ìœ„ì¹˜

### 2. ì„œìš¸ìº í¼ìŠ¤ í¬ë¡¤ë§ êµ¬í˜„

`crawler.py`ì˜ `_crawl_seoul_campus()` ë©”ì„œë“œì—ì„œ:

```python
def _crawl_seoul_campus(self, target_date: date) -> List[Menu]:
    menus = []
    response = requests.get(self.seoul_url, headers=self.headers, timeout=10)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # ì˜ˆì‹œ: ë©”ë‰´ í…Œì´ë¸” ì°¾ê¸°
    menu_table = soup.find('table', class_='ì‹ë‹¨í‘œ_í´ë˜ìŠ¤ëª…')
    if not menu_table:
        return menus
    
    # ìš”ì¼ë³„ë¡œ íŒŒì‹±
    rows = menu_table.find_all('tr')
    for row in rows:
        cells = row.find_all('td')
        # ë‚ ì§œ, ì‹ì‚¬íƒ€ì…, ë©”ë‰´ ì¶”ì¶œ
        # ...
    
    return menus
```

### 3. ì²œì•ˆìº í¼ìŠ¤ ì´ë¯¸ì§€ ê¸°ë°˜ í¬ë¡¤ë§

ê²Œì‹œíŒì—ì„œ ìµœì‹  ê²Œì‹œë¬¼ì˜ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  OCR ì²˜ë¦¬:

```python
def _crawl_cheonan_faculty(self, target_date: date) -> List[Menu]:
    menus = []
    response = requests.get(self.cheonan_faculty_url, headers=self.headers, timeout=10)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # ê²Œì‹œê¸€ ëª©ë¡ì—ì„œ ìµœì‹  ê¸€ ì°¾ê¸°
    post_link = soup.find('a', class_='ê²Œì‹œê¸€_ë§í¬_í´ë˜ìŠ¤')
    if not post_link:
        return menus
    
    # ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
    post_url = self.base_url + post_link['href']
    post_response = requests.get(post_url, headers=self.headers, timeout=10)
    post_soup = BeautifulSoup(post_response.content, 'html.parser')
    
    # ì´ë¯¸ì§€ URL ì¶”ì¶œ
    image_urls = self._extract_image_urls(post_soup)
    
    # ê° ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    for img_url in image_urls:
        text = self._extract_text_from_image(img_url)
        if text:
            menu = self._parse_menu_text(text, target_date, Restaurant.CHEONAN_FACULTY)
            if menu:
                menus.append(menu)
    
    return menus
```

### 4. OCR ì„¤ì • (Tesseract)

Tesseract OCRì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:

```bash
# ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
cd backend
./install_tesseract.sh

# ë˜ëŠ” ìˆ˜ë™ ì„¤ì¹˜
# Ubuntu/Debian
sudo apt-get install tesseract-ocr tesseract-ocr-kor

# Mac
brew install tesseract tesseract-lang

# Windows
# https://github.com/UB-Mannheim/tesseract/wiki ì°¸ì¡°
```

### 5. í…ŒìŠ¤íŠ¸

```python
# í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
from crawler import SMUCafeteriaCrawler
from datetime import date

crawler = SMUCafeteriaCrawler()

# ì˜¤ëŠ˜ ë©”ë‰´ í¬ë¡¤ë§
menus = crawler.crawl_daily_menu(date.today())
print(f"Crawled {len(menus)} menus")
for menu in menus:
    print(f"{menu.restaurant} - {menu.meal_type}")
    for item in menu.items:
        print(f"  {item.name}: {item.price}ì›")
```

### 6. ì£¼ì˜ì‚¬í•­

- **robots.txt í™•ì¸**: í¬ë¡¤ë§ì´ í—ˆìš©ë˜ëŠ”ì§€ í™•ì¸
- **ìš”ì²­ ê°„ê²©**: ë„ˆë¬´ ë¹ˆë²ˆí•œ ìš”ì²­ìœ¼ë¡œ ì„œë²„ì— ë¶€ë‹´ ì£¼ì§€ ì•Šê¸°
- **ì—ëŸ¬ ì²˜ë¦¬**: ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ ì‹œ ëŒ€ë¹„
- **í•œêµ­ì–´ OCR**: Tesseract í•œêµ­ì–´ ë°ì´í„° í•„ìˆ˜

### 7. ë””ë²„ê¹…

```python
# HTML êµ¬ì¡° í™•ì¸
import requests
from bs4 import BeautifulSoup

url = "https://www.smu.ac.kr/kor/life/restaurantView.do"
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

# ëª¨ë“  í…Œì´ë¸” ì¶œë ¥
tables = soup.find_all('table')
print(f"Found {len(tables)} tables")

# í´ë˜ìŠ¤ê°€ ìˆëŠ” div ì¶œë ¥
divs = soup.find_all('div', class_=True)
for div in divs[:10]:
    print(div.get('class'))
```

### 8. ì¶”ê°€ ê¸°ëŠ¥ ì•„ì´ë””ì–´

- ë©”ë‰´ ë³€ê²½ ì•Œë¦¼
- ì¸ê¸° ë©”ë‰´ í†µê³„
- ë©”ë‰´ í‰ì  ì‹œìŠ¤í…œ
- ì˜ì–‘ ì •ë³´ ì¶”ê°€
- ë©”ë‰´ ê²€ìƒ‰ ê¸°ëŠ¥
